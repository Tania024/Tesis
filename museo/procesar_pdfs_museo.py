"""
PROCESADOR DE PDFs DEL MUSEO PUMAPUNGO
Extrae informaci√≥n de los PDFs y la estructura por √°reas usando IA local (Ollama)

INSTRUCCIONES:
1. Copia este archivo a: C:\\Users\\Tania\\Documents\\Tesis\\museo\\backend\\
2. Pon todos los PDFs en: C:\\Users\\Tania\\Documents\\Tesis\\museo\\pdfs_museo\\
3. Ejecuta: python procesar_pdfs_museo.py
4. Se generar√°: museo_knowledge.json
"""

import os
import json
import re
from pathlib import Path
from typing import List, Dict, Any
import requests
from datetime import datetime

# Instalar si no tienes: pip install pypdf pdfplumber
try:
    import pdfplumber
    print("‚úÖ pdfplumber disponible")
except ImportError:
    print("‚ùå Instala pdfplumber: pip install pdfplumber")
    exit(1)

# Configuraci√≥n
CARPETA_PDFS = r"C:\Users\Tania\Documents\Tesis\museo\pdfs_museo"
OLLAMA_URL = "http://localhost:11434"
MODELO = "deepseek-r1:7b"
OUTPUT_FILE = "museo_knowledge.json"

# √Åreas del Museo Pumapungo (basado en tu BD)
AREAS_MUSEO = {
    "ARQ-01": "Sala Arqueol√≥gica Ca√±ari",
    "ETN-01": "Sala Etnogr√°fica",
    "AVE-01": "Aviario de Aves Andinas",
    "BOT-01": "Jard√≠n Bot√°nico",
    "ART-01": "Sala de Arte Colonial",
    "RUIN-01": "Parque Arqueol√≥gico Pumapungo",
    "TEMP-01": "Exhibici√≥n Temporal"
}


def extraer_texto_pdf(ruta_pdf: str) -> str:
    """Extraer texto de un PDF"""
    try:
        print(f"  üìÑ Leyendo: {os.path.basename(ruta_pdf)}")
        texto_completo = []
        
        with pdfplumber.open(ruta_pdf) as pdf:
            print(f"     P√°ginas: {len(pdf.pages)}")
            
            for i, pagina in enumerate(pdf.pages, 1):
                texto = pagina.extract_text()
                if texto:
                    texto_completo.append(texto)
                
                # Mostrar progreso cada 10 p√°ginas
                if i % 10 == 0:
                    print(f"     Procesadas {i}/{len(pdf.pages)} p√°ginas...")
        
        texto_final = "\n\n".join(texto_completo)
        print(f"  ‚úÖ Extra√≠dos {len(texto_final)} caracteres")
        return texto_final
    
    except Exception as e:
        print(f"  ‚ùå Error: {e}")
        return ""


def dividir_texto_en_chunks(texto: str, max_chars: int = 15000) -> List[str]:
    """Dividir texto largo en chunks manejables"""
    # Dividir por p√°rrafos
    parrafos = texto.split('\n\n')
    chunks = []
    chunk_actual = ""
    
    for parrafo in parrafos:
        if len(chunk_actual) + len(parrafo) < max_chars:
            chunk_actual += parrafo + "\n\n"
        else:
            if chunk_actual:
                chunks.append(chunk_actual)
            chunk_actual = parrafo + "\n\n"
    
    if chunk_actual:
        chunks.append(chunk_actual)
    
    return chunks


def analizar_texto_con_ia(texto: str, nombre_pdf: str) -> Dict[str, Any]:
    """Usar Ollama para extraer informaci√≥n estructurada del texto"""
    
    # Dividir si es muy largo
    chunks = dividir_texto_en_chunks(texto, max_chars=15000)
    
    if len(chunks) > 1:
        print(f"  üîç Analizando {len(chunks)} secciones con IA...")
    else:
        print(f"  üîç Analizando texto con IA...")
    
    # Lista de √°reas para el prompt
    areas_lista = "\n".join([f"- {codigo}: {nombre}" for codigo, nombre in AREAS_MUSEO.items()])
    
    informacion_extraida = {
        "areas": {},
        "general": []
    }
    
    for i, chunk in enumerate(chunks, 1):
        if len(chunks) > 1:
            print(f"     Secci√≥n {i}/{len(chunks)}...")
        
        prompt = f"""Eres un experto analizando documentaci√≥n del Museo Pumapungo de Cuenca, Ecuador.

√ÅREAS DEL MUSEO:
{areas_lista}

TEXTO A ANALIZAR:
{chunk[:12000]}  # Limitar tama√±o

TAREA: Extrae informaci√≥n relevante y clasif√≠cala por √°rea del museo.

RESPONDE SOLO CON JSON V√ÅLIDO (sin texto adicional):

{{
  "areas_identificadas": [
    {{
      "area_codigo": "SPN",
      "segmentos_texto": [
        "Texto relevante encontrado sobre esta √°rea...",
        "Otro segmento relevante..."
      ],
      "temas": ["tema1", "tema2"],
      "objetos_mencionados": ["objeto1", "objeto2"],
      "datos_historicos": ["dato1", "dato2"]
    }}
  ],
  "informacion_general": [
    "Informaci√≥n que no corresponde a ning√∫n √°rea espec√≠fica"
  ]
}}

REGLAS:
1. Solo JSON v√°lido, sin texto antes o despu√©s
2. Solo incluye informaci√≥n que realmente aparece en el texto
3. Identifica claramente a qu√© √°rea pertenece cada informaci√≥n
4. Si no est√°s seguro del √°rea, ponlo en "informacion_general"
"""
        
        try:
            response = requests.post(
                f"{OLLAMA_URL}/api/generate",
                json={
                    "model": MODELO,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.1,  # M√°s determin√≠stico
                        "num_predict": 4000
                    }
                },
                timeout=300  # 5 minutos
            )
            
            if response.status_code == 200:
                resultado = response.json()
                respuesta_ia = resultado.get("response", "")
                
                # Extraer JSON
                try:
                    # Intentar parsear directamente
                    data = json.loads(respuesta_ia)
                except:
                    # Buscar JSON dentro del texto
                    match = re.search(r'\{.*\}', respuesta_ia, re.DOTALL)
                    if match:
                        data = json.loads(match.group(0))
                    else:
                        print(f"     ‚ö†Ô∏è No se pudo extraer JSON de la secci√≥n {i}")
                        continue
                
                # Agregar informaci√≥n extra√≠da
                if "areas_identificadas" in data:
                    for area_info in data["areas_identificadas"]:
                        codigo = area_info.get("area_codigo")
                        if codigo in AREAS_MUSEO:
                            if codigo not in informacion_extraida["areas"]:
                                informacion_extraida["areas"][codigo] = {
                                    "nombre": AREAS_MUSEO[codigo],
                                    "segmentos": [],
                                    "temas": [],
                                    "objetos": [],
                                    "datos_historicos": []
                                }
                            
                            # Agregar informaci√≥n
                            informacion_extraida["areas"][codigo]["segmentos"].extend(
                                area_info.get("segmentos_texto", [])
                            )
                            informacion_extraida["areas"][codigo]["temas"].extend(
                                area_info.get("temas", [])
                            )
                            informacion_extraida["areas"][codigo]["objetos"].extend(
                                area_info.get("objetos_mencionados", [])
                            )
                            informacion_extraida["areas"][codigo]["datos_historicos"].extend(
                                area_info.get("datos_historicos", [])
                            )
                
                if "informacion_general" in data:
                    informacion_extraida["general"].extend(data["informacion_general"])
                
                print(f"     ‚úÖ Secci√≥n {i} analizada")
            
            else:
                print(f"     ‚ùå Error en API: {response.status_code}")
        
        except Exception as e:
            print(f"     ‚ùå Error analizando secci√≥n {i}: {e}")
            continue
    
    return informacion_extraida


def procesar_todos_los_pdfs():
    """Procesar todos los PDFs de la carpeta"""
    
    print("=" * 80)
    print("üèõÔ∏è  PROCESADOR DE PDFs - MUSEO PUMAPUNGO")
    print("=" * 80)
    print()
    
    # Verificar carpeta
    if not os.path.exists(CARPETA_PDFS):
        print(f"‚ùå La carpeta no existe: {CARPETA_PDFS}")
        print(f"   Crea la carpeta y pon los PDFs ah√≠")
        return
    
    # Buscar PDFs
    pdfs = list(Path(CARPETA_PDFS).glob("*.pdf"))
    
    if not pdfs:
        print(f"‚ùå No se encontraron PDFs en: {CARPETA_PDFS}")
        return
    
    print(f"üìö Encontrados {len(pdfs)} PDFs")
    print()
    
    # Verificar Ollama
    try:
        response = requests.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        if response.status_code != 200:
            print("‚ùå Ollama no est√° corriendo")
            print("   Inicia Ollama primero")
            return
        print("‚úÖ Ollama conectado")
    except:
        print("‚ùå No se puede conectar con Ollama")
        print("   Aseg√∫rate de que Ollama est√© corriendo")
        return
    
    print()
    print("üöÄ Iniciando procesamiento...")
    print()
    
    # Base de conocimiento
    knowledge_base = {
        "museo": "Museo Pumapungo",
        "ubicacion": "Cuenca, Ecuador",
        "fecha_procesamiento": datetime.now().isoformat(),
        "total_pdfs": len(pdfs),
        "areas": {}
    }
    
    # Inicializar √°reas
    for codigo, nombre in AREAS_MUSEO.items():
        knowledge_base["areas"][codigo] = {
            "codigo": codigo,
            "nombre": nombre,
            "descripcion": "",
            "historia": "",
            "objetos_destacados": [],
            "datos_curiosos": [],
            "temas_principales": [],
            "informacion_detallada": []
        }
    
    # Procesar cada PDF
    for i, pdf_path in enumerate(pdfs, 1):
        print(f"üìñ PDF {i}/{len(pdfs)}: {pdf_path.name}")
        print("-" * 80)
        
        # Extraer texto
        texto = extraer_texto_pdf(str(pdf_path))
        
        if not texto or len(texto) < 100:
            print("  ‚ö†Ô∏è PDF vac√≠o o sin texto extra√≠ble")
            print()
            continue
        
        # Analizar con IA
        info_extraida = analizar_texto_con_ia(texto, pdf_path.name)
        
        # Agregar a knowledge base
        for codigo, area_info in info_extraida["areas"].items():
            if codigo in knowledge_base["areas"]:
                knowledge_base["areas"][codigo]["informacion_detallada"].extend(
                    area_info["segmentos"]
                )
                knowledge_base["areas"][codigo]["objetos_destacados"].extend(
                    area_info["objetos"]
                )
                knowledge_base["areas"][codigo]["datos_curiosos"].extend(
                    area_info["datos_historicos"]
                )
                knowledge_base["areas"][codigo]["temas_principales"].extend(
                    area_info["temas"]
                )
        
        print()
    
    # Limpiar duplicados
    print("üßπ Limpiando duplicados...")
    for codigo in knowledge_base["areas"]:
        area = knowledge_base["areas"][codigo]
        area["objetos_destacados"] = list(set(area["objetos_destacados"]))
        area["datos_curiosos"] = list(set(area["datos_curiosos"]))
        area["temas_principales"] = list(set(area["temas_principales"]))
    
    # Guardar resultado
    print(f"üíæ Guardando resultado en: {OUTPUT_FILE}")
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(knowledge_base, f, indent=2, ensure_ascii=False)
    
    print()
    print("=" * 80)
    print("‚úÖ PROCESAMIENTO COMPLETADO")
    print("=" * 80)
    print()
    print(f"üìä Resumen:")
    print(f"   - PDFs procesados: {len(pdfs)}")
    print(f"   - √Åreas con informaci√≥n:")
    for codigo, area in knowledge_base["areas"].items():
        total_info = (
            len(area["informacion_detallada"]) +
            len(area["objetos_destacados"]) +
            len(area["datos_curiosos"])
        )
        if total_info > 0:
            print(f"     ‚Ä¢ {codigo} - {area['nombre']}: {total_info} items")
    
    print()
    print(f"üìÅ Archivo generado: {OUTPUT_FILE}")
    print(f"   C√≥pialo a tu carpeta del backend para usarlo en el sistema")
    print()


if __name__ == "__main__":
    procesar_todos_los_pdfs()